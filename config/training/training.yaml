# config/training.yaml
batch_size: 4
epochs: 50
learning_rate: 1e-5
weight_decay: 1e-7
optimizer: "Adam"
omega: 0.95           # Weight for teacher vs. ground truth in representation distillation
temperature: 0.1      # For logit distillation
logging_interval: 10  # Log every N batches
num_workers: 4